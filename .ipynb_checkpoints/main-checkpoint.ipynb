{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df53ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def loadMatlabData(filePath):\n",
    "    fileName = filePath + 'stretch_press_data.mat'\n",
    "    \n",
    "    ###============= Load Matlab files\n",
    "    contentsMat = sio.loadmat(fileName)\n",
    "    x_data = contentsMat['x_data']\n",
    "    y_data = contentsMat['y_data']\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "def dnn_model():\n",
    "    input1 = tf.keras.layers.Input(shape=(100,1,1), name='stretch_press')\n",
    "    input2 = tf.keras.layers.Input(shape=(1), name='amplitude')\n",
    "    input3 = tf.keras.layers.Input(shape=(1), name='time')\n",
    "    \n",
    "    x_concat1 = same_model(input1)\n",
    "    x_concat2 = input2\n",
    "    x_concat3 = input3\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([x_concat1,x_concat2,x_concat3])\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='FC1')(x)\n",
    "    output = tf.keras.layers.Dense(8, activation='softmax', name='Output')(x)\n",
    "    \n",
    "    model= tf.keras.models.Model(inputs=[input1, input2, input3], outputs=output)\n",
    "    #model = tf.keras.applications.resnet.ResNet50(weights=None, input_tensor=tf.keras.layers.Input(shape=(100, 1, 1)), classes=8)\n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n",
    "def same_model(input):\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=(2,1), activation = 'relu', padding='same', name='CV1')(input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size = (3,1), strides = (2,1), name='MP1')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(2,1), activation = 'relu', padding='same', name='CV2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size = (3,1), strides = (2,1), name='MP2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Flatten(name='Flatten')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def one_hot(y_, n_classes=6):\n",
    "    # Function to encode neural one-hot output labels from number indexes\n",
    "    # e.g.:\n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS \n",
    "\n",
    "def decode_one_hot(y_, n_classes=6):\n",
    "    new_y = np.zeros([int(y_.size/n_classes)])\n",
    "    for i in range(0,int(y_.size/n_classes)):\n",
    "        max = 0\n",
    "        for j in range(0,n_classes):\n",
    "            if(y_[i,max] < y_[i,j]):\n",
    "                max = j\n",
    "        new_y[i] = max;\n",
    "        \n",
    "    return new_y  # Returns FLOATS \n",
    "\n",
    "def reshape_input(x_train, y_train, x_valid, y_valid, x_test, y_test):\n",
    "    \n",
    "    x_train = x_train.reshape(int(x_train.size/102),102,1)\n",
    "    y_train = y_train.reshape(int(y_train.size),1,1)\n",
    "    x_valid = x_valid.reshape(int(x_valid.size/102),102,1)\n",
    "    y_valid = y_valid.reshape(int(y_valid.size),1,1) \n",
    "    x_test = x_test.reshape(int(x_test.size/102),102,1)\n",
    "    y_test = y_test.reshape(int(y_test.size),1,1) \n",
    "                \n",
    "    train_time = x_train[:, 100, :].flatten()\n",
    "    valid_time = x_valid[:, 100, :].flatten()\n",
    "    test_time = x_test[:, 100, :].flatten()\n",
    "    \n",
    "    train_time = train_time.reshape(train_time.size,1)\n",
    "    valid_time = valid_time.reshape(valid_time.size,1)\n",
    "    test_time = test_time.reshape(test_time.size,1)\n",
    "    \n",
    "    ss1 = sklearn.preprocessing.StandardScaler()\n",
    "    ss1.fit(train_time)\n",
    "    train_time = ss1.transform(train_time)\n",
    "    valid_time = ss1.transform(valid_time)\n",
    "    test_time = ss1.transform(test_time)\n",
    "    \n",
    "    train_amplitude = x_train[:, 101, :].flatten()\n",
    "    valid_amplitude = x_valid[:, 101, :].flatten()\n",
    "    test_amplitude = x_test[:, 101, :].flatten()\n",
    "    \n",
    "    train_amplitude = train_amplitude.reshape(train_amplitude.size,1)\n",
    "    valid_amplitude = valid_amplitude.reshape(valid_amplitude.size,1)\n",
    "    test_amplitude = test_amplitude.reshape(test_amplitude.size,1)\n",
    "    \n",
    "    ss2 = sklearn.preprocessing.StandardScaler()\n",
    "    ss2.fit(train_amplitude)\n",
    "    train_amplitude = ss2.transform(train_amplitude)\n",
    "    valid_amplitude = ss2.transform(valid_amplitude)\n",
    "    test_amplitude = ss2.transform(test_amplitude)\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test, train_time, valid_time, test_time, train_amplitude, valid_amplitude, test_amplitude\n",
    "    \n",
    "def K_fold(x_train, y_train, x_test, y_test):\n",
    "    num_folds = 0\n",
    "    str_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=44)\n",
    "    accs = []\n",
    "    y_train = y_train-1\n",
    "    y_test = y_test-1\n",
    "    \n",
    "    for train_idx, valid_idx in str_kf.split(x_train, y_train):\n",
    "        num_folds += 1\n",
    "        print(f'--------------------{num_folds}번째 KFold-------------------')\n",
    "        print(f'train_idx_len : {len(train_idx)} / valid_idx_len : {len(valid_idx)}')\n",
    "\n",
    "        data_train, data_valid = x_train[train_idx], x_train[valid_idx]\n",
    "        label_train, label_valid = y_train[train_idx], y_train[valid_idx]\n",
    "        \n",
    "        # Data augmentation\n",
    "        aug_xtrain, aug_ytrain = data_aug(data_train,label_train, [0, 1, 2, 3, 4, 5, 6, 7])\n",
    "                    \n",
    "        # Data shuffle\n",
    "        tmp = [[x,y] for x,y in zip(aug_xtrain,aug_ytrain)]\n",
    "        random.shuffle(tmp)\n",
    "        aug_xtrain = [n[0] for n in tmp]\n",
    "        aug_ytrain = [n[1] for n in tmp]\n",
    "        aug_xtrain = np.array(aug_xtrain)\n",
    "        aug_ytrain = np.array(aug_ytrain)\n",
    "        \n",
    "        aug_xtrain, aug_ytrain, data_valid, label_valid, x_test1, y_test1, train_time, valid_time, test_time, train_amplitude, valid_amplitude, test_amplitude = reshape_input(aug_xtrain, aug_ytrain, data_valid, label_valid, x_test, y_test)\n",
    "        \n",
    "        #model\n",
    "        model = dnn_model()\n",
    "        \n",
    "        callback_list = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath='model_best_fold' + str(num_folds) + '.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True),\n",
    "        ]\n",
    "\n",
    "        Input1 = aug_xtrain[:, 0:100, :]\n",
    "        Input2 = aug_xtrain[:, 100, :].flatten()\n",
    "        Input3 = aug_xtrain[:, 101, :].flatten()\n",
    "        Val_Input1 = data_valid[:, 0:100, :]\n",
    "        Val_Input2 = data_valid[:, 100, :].flatten()\n",
    "        Val_Input3 = data_valid[:, 101, :].flatten()\n",
    "        \n",
    "        Input2 = train_time\n",
    "        Input3 = train_amplitude\n",
    "        Val_Input2 = valid_time\n",
    "        Val_Input3 = valid_amplitude\n",
    "                       \n",
    "        model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        model.fit([Input1, Input2, Input3], aug_ytrain, batch_size=32, validation_data = ([Val_Input1, Val_Input2, Val_Input3],label_valid) , epochs=100, callbacks=callback_list)\n",
    "        vPred = model.predict([x_test1[:,0:100,:], test_time, test_amplitude])        \n",
    "        loss, acc = model.evaluate([x_test1[:,0:100,:], test_time, test_amplitude], y_test1)\n",
    "        accs.append(acc)\n",
    "        print(acc)\n",
    "        \n",
    "        Pred = decode_one_hot(vPred,8)\n",
    "        result = pd.DataFrame({'Pred': Pred, 'label':y_test1.flatten()})\n",
    "        result.to_csv(\"./Stretch_press/Result/\" + 'Result'+str(num_folds)+'.csv')\n",
    "        \n",
    "        Pred = decode_one_hot(vPred)\n",
    "        # Pred = one_hot(Pred)\n",
    "                \n",
    "        accuracy, precision, recall, spec, roc_auc, bal_acc, f1 = get_clf_eval(label_valid, Pred, vPred, [0, 1, 2, 3, 4, 5, 6, 7])\n",
    "        \n",
    "    return aug_xtrain, aug_ytrain, x_test1, y_test1, test_time, test_amplitude\n",
    "        \n",
    "def data_aug(x_data,y_data, labels):\n",
    "    random.seed(1)\n",
    "    \n",
    "    # find maximum\n",
    "    max_count = 0\n",
    "    for i in labels:\n",
    "        print(np.size(np.where(y_data == i)))\n",
    "        if(max_count < np.size(np.where(y_data == i)[0])):\n",
    "            max_count = np.size(np.where(y_data == i)[0])\n",
    "    \n",
    "    \n",
    "    # augmentation\n",
    "    new_data = np.zeros((1,102))\n",
    "    new_ydata = np.zeros((1,1))\n",
    "    for i in labels:\n",
    "        aug_num = max_count - np.size(np.where(y_data == i)[0])\n",
    "        aug_data = np.where(y_data == i)[0]\n",
    "        for j in range(aug_num):\n",
    "            rand_aug = random.randrange(np.shape(aug_data)[0])\n",
    "            rand_num = random.randrange(-5,6)\n",
    "            if(rand_num < 0):\n",
    "                new_data[0][0:-rand_num] = x_data[aug_data[rand_aug]][100+rand_num:100]\n",
    "                new_data[0][-rand_num:100] = x_data[aug_data[rand_aug]][0:100+rand_num]\n",
    "                new_data[0][100:102] = x_data[aug_data[rand_aug]][100:102]\n",
    "            else : \n",
    "                new_data[0][0:100-rand_num] = x_data[aug_data[rand_aug]][rand_num:100]\n",
    "                new_data[0][100-rand_num:100] = x_data[aug_data[rand_aug]][0:rand_num]\n",
    "                new_data[0][100:102] = x_data[aug_data[rand_aug]][100:102]\n",
    "            x_data = np.concatenate((x_data,new_data))\n",
    "            new_ydata[0][0] = i\n",
    "            y_data = np.concatenate((y_data,new_ydata))\n",
    "    \n",
    "    return x_data, y_data   \n",
    "        \n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None, labels=[0, 1]):\n",
    "    confusion = confusion_matrix(y_test, pred, labels)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    spec = []\n",
    "    bal_acc = []\n",
    "    recall = []\n",
    "    for i in labels:\n",
    "        confusion_col = 0\n",
    "        confusion_row = 0\n",
    "        for j in labels:\n",
    "            confusion_col += confusion[i,j]\n",
    "            confusion_row += confusion[j,i]\n",
    "        spec.append(confusion[i,i] / confusion_col)\n",
    "        recall.append(confusion[i,i] / confusion_row)\n",
    "        bal_acc.append((recall[i] + spec[i]) / 2)\n",
    "    \n",
    "    \n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율(Sensitivity): {2:.4f}, 특이성(Specificity): {3:.4f}, F1: {4:.4f}, AUC: {5:.4f}, bal_acc: {6:.4f}'.format(accuracy, precision, recall, spec, f1, roc_auc, bal_acc))\n",
    "    return accuracy, precision, recall, spec, roc_auc, bal_acc, f1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22ebb7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Stretch_press/data/stretch_press_data.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\Stretch_press\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Stretch_press/data/stretch_press_data.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_data, y_data \u001b[38;5;241m=\u001b[39m \u001b[43mloadMatlabData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Stretch_press/data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x_data, y_data,test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_data , shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# result = pd.DataFrame(x_train.squeeze(),y_train.squeeze())\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# result.to_csv(\"./Stretch_press/Result/\" + 'Result1.csv')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#### Kfold\u001b[39;00m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mloadMatlabData\u001b[1;34m(filePath)\u001b[0m\n\u001b[0;32m     14\u001b[0m fileName \u001b[38;5;241m=\u001b[39m filePath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstretch_press_data.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m###============= Load Matlab files\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m contentsMat \u001b[38;5;241m=\u001b[39m \u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m x_data \u001b[38;5;241m=\u001b[39m contentsMat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m y_data \u001b[38;5;241m=\u001b[39m contentsMat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Stretch_press\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    225\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    226\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Stretch_press\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Stretch_press\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Stretch_press\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Stretch_press/data/stretch_press_data.mat'"
     ]
    }
   ],
   "source": [
    "x_data, y_data = loadMatlabData(\"./Stretch_press/data/\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data,test_size = 0.2, stratify=y_data , shuffle=True, random_state=99)\n",
    "\n",
    "# result = pd.DataFrame(x_train.squeeze(),y_train.squeeze())\n",
    "# result.to_csv(\"./Stretch_press/Result/\" + 'Result1.csv')\n",
    "\n",
    "#### Kfold\n",
    "x_train, y_train, x_test, y_test, test_time, test_amplitude = K_fold(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# result = pd.DataFrame(x_train.squeeze(),y_train.squeeze())\n",
    "# result.to_csv(\"./Stretch_press/Result/\" + 'Result1.csv')\n",
    "\n",
    "#### load weight\n",
    "model = dnn_model()\n",
    "model.load_weights('model_best_fold2.h5')\n",
    "vPred = model.predict([x_test[:, 0:100, :], test_time, test_amplitude]).squeeze()\n",
    "Pred = decode_one_hot(vPred,8)\n",
    "result = pd.DataFrame({'Pred': Pred, 'label':y_test.flatten()})\n",
    "result.to_csv(\"./Stretch_press/Result/\" + 'Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1448b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stretch_press",
   "language": "python",
   "name": "stretch_press"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
